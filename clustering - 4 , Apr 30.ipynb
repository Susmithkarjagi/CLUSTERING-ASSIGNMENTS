{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa79af8c-ac05-4a69-9693-40b3ea78e6ac",
   "metadata": {},
   "source": [
    "## Assignment on Clustering - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f70410-73a7-4690-a210-bc5107c84e3b",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of homogeneity and completeness in clustering evaluation. How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e343c-5c9f-4b91-aab5-1ac40a2f6efa",
   "metadata": {},
   "source": [
    "Homogeneity: A clustering result is said to be homogeneous if all of its clusters contain only data points which are members of a single class. In other words, each cluster should contain elements of only one class. A homogeneity score of 1 indicates perfectly homogeneous labeling, while a score closer to 0 indicates less homogeneous labeling.\n",
    "\n",
    "Completeness: A clustering result is said to be complete if all the data points that are members of a given class are elements of the same cluster. This means that elements belonging to the same class should all fall into the same cluster. A completeness score of 1 indicates perfectly complete labeling, while a score closer to 0 indicates less complete labeling.\n",
    "\n",
    "The calculation of homogeneity and completeness scores is based on counting pairs of samples:\n",
    "\n",
    "Same cluster, same class (a)\n",
    "Same cluster, different class (b)\n",
    "Different cluster, same class (c)\n",
    "Different cluster, different class (d)\n",
    "The homogeneity score (h) is given by:\n",
    "\n",
    "h = 1 - H(C|K) / H(C)\n",
    "\n",
    "Where H(C|K) is the conditional entropy of the classes given the cluster assignments, and H(C) is the entropy of the classes.\n",
    "\n",
    "The completeness score (c) is given by:\n",
    "\n",
    "c = 1 - H(K|C) / H(K)\n",
    "\n",
    "Where H(K|C) is the conditional entropy of the clusters given the class assignments, and H(K) is the entropy of the clusters.\n",
    "\n",
    "Entropy is a measure of uncertainty. Conditional entropy H(C|K) measures the average uncertainty of class labels given the cluster assignments, while H(K|C) measures the average uncertainty of cluster assignments given the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6129f-9368-4ccd-8b73-47fa1e48549a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33dea292-e6b0-4aa8-a2b0-f160b97f5a87",
   "metadata": {},
   "source": [
    "Q2. What is the V-measure in clustering evaluation? How is it related to homogeneity and completeness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c639d8-6e07-4ef7-a12c-898bab60ac8f",
   "metadata": {},
   "source": [
    "The V-measure is an external clustering evaluation metric that combines both homogeneity (h) and completeness (c). It is defined as the harmonic mean of homogeneity and completeness.\n",
    "\n",
    "The V-measure is given by:\n",
    "\n",
    "V = 2 * (h * c) / (h + c)\n",
    "\n",
    "This measure is especially useful in situations where both homogeneity and completeness are important for a good clustering, and neither should be sacrificed at the expense of the other. The V-measure value ranges between 0 and 1, where 1 indicates perfect agreement between the true and predicted clustering, and 0 indicates no agreement at all.\n",
    "\n",
    "By using the harmonic mean, the V-measure punishes extreme values. This means that if either homogeneity or completeness is low, it will pull down the V-measure score significantly. So, a high V-measure score can be achieved only if both homogeneity and completeness are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c6f62-2de0-4a3e-b0aa-7bd8eba8fd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7b5d078-548a-492a-86d3-852b8a454092",
   "metadata": {},
   "source": [
    "Q3. How is the Silhouette Coefficient used to evaluate the quality of a clustering result? What is the range of its values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29f6d1-b6be-4a00-9817-b7395baded69",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is an internal evaluation metric for clustering that measures how well each datapoint fits within its assigned cluster and how poorly it fits into neighboring clusters. It doesn't require ground truth labels, so it's particularly useful for assessing the quality of unsupervised algorithms like DBSCAN, K-means, and hierarchical clustering.\n",
    "\n",
    "The Silhouette Coefficient for each data point is calculated using the average intra-cluster distance (a) and the average nearest-cluster distance (b) for that point. The average intra-cluster distance is the average distance between the point and all other points within the same cluster. The average nearest-cluster distance is the average distance from the point to the points in the nearest cluster to which it doesn't belong.\n",
    "\n",
    "The Silhouette Coefficient (s) for a single sample is given by the formula:\n",
    "\n",
    "s = (b - a) / max(a, b)\n",
    "\n",
    "For a set of samples, the Silhouette Coefficient is given as the mean of the Silhouette Coefficient for each sample.\n",
    "\n",
    "The value of the Silhouette Coefficient ranges between -1 and 1. A high value close to 1 indicates that the sample is far away from the neighboring clusters and close to the points within its own cluster, which is a desirable property. A value close to 0 means the point is on or very close to the decision boundary between two neighboring clusters. A negative value indicates that the sample might have been assigned to the wrong cluster, as it is closer to a neighboring cluster than to the one it was assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a1396-2e2f-4083-bb49-e3bc2a53f819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46c7e6bc-72ad-47fe-a50f-0205dddfcedb",
   "metadata": {},
   "source": [
    "Q4. How is the Davies-Bouldin Index used to evaluate the quality of a clustering result? What is the rangeof its values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9775b-0faa-45c5-be7e-8072a8121104",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index (DBI) is an internal evaluation metric for clustering. It measures the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.\n",
    "\n",
    "For each cluster, the DBI computes a measure (Rij) of the 'similarity' of cluster i to cluster j, which is the ratio of the sum of the average distances of points in clusters i and j to their respective centroids, to the maximum of the average distances of points in clusters i and j to their respective centroids.\n",
    "\n",
    "The DBI for a given cluster is then the maximum Rij across all j (j ≠ i). The DBI for the entire clustering is the average DBI for all clusters.\n",
    "\n",
    "The DBI is given by the formula:\n",
    "\n",
    "DBI = (1/n) * Σ (Max (i ≠ j) (Si + Sj) / d(ci, cj))\n",
    "\n",
    "where:\n",
    "\n",
    "n is the number of clusters\n",
    "Si is the average distance of all elements in cluster i to the centroid ci\n",
    "Sj is the average distance of all elements in cluster j to the centroid cj\n",
    "d(ci, cj) is the distance between centroids ci and cj\n",
    "\n",
    "The lower the DBI, the better. The minimum value of the DBI is 0, which indicates the optimal clustering. A value of 0 means that clusters are compact (i.e., the distances between the points in a cluster and their centroid are small) and far from each other. The DBI doesn't have a specified maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b9cf9-a97d-4102-87ff-bbf4426ecfc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02756f19-1548-464f-b0b9-4b09f3de5850",
   "metadata": {},
   "source": [
    "Q5. Can a clustering result have a high homogeneity but low completeness? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629b83-da81-47e9-b22b-2c4526832aa8",
   "metadata": {},
   "source": [
    "Yes, it's indeed possible for a clustering result to have high homogeneity but low completeness. These two metrics evaluate different aspects of a clustering result.\n",
    "\n",
    "High Homogeneity: If a clustering result has high homogeneity, it means that each cluster contains only members of a single class. Homogeneity is maximized when each cluster contains data points that belong to a single class.\n",
    "\n",
    "Low Completeness: On the other hand, low completeness means that members of a given class are scattered across multiple clusters. Completeness is maximized when all data points that are members of a given class are elements of the same cluster.\n",
    "\n",
    "Let's consider an example where you are clustering a dataset of animals, and the classes are 'Dogs', 'Cats', and 'Birds'. You applied a clustering algorithm that generated 6 clusters with the following compositions:\n",
    "\n",
    "Cluster 1: Dogs\n",
    "Cluster 2: Dogs\n",
    "Cluster 3: Cats\n",
    "Cluster 4: Cats\n",
    "Cluster 5: Birds\n",
    "Cluster 6: Birds\n",
    "In this case, the homogeneity is high because each cluster only contains data points from a single class. So, the clusters are homogeneous.\n",
    "\n",
    "However, the completeness is low because members of the same class ('Dogs', 'Cats', and 'Birds') are scattered across multiple clusters. Therefore, even though the clusters are pure (high homogeneity), they don't capture all members of the classes (low completeness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee90031-a5cc-4426-b113-cb0158592cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "47a33707-dc2e-4777-8d67-093a80134ccf",
   "metadata": {},
   "source": [
    "Q6. How can the V-measure be used to determine the optimal number of clusters in a clustering algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61981509-6d1f-4b54-bd1a-05606c4cb984",
   "metadata": {},
   "source": [
    "The V-measure is an external evaluation metric that provides a balanced assessment of the quality of a clustering result by combining both homogeneity and completeness into a single measure. This can be used to determine the optimal number of clusters in a dataset when true labels are available.\n",
    "\n",
    "The process would typically involve running the clustering algorithm multiple times, each time with a different number of clusters, and then computing the V-measure for each result. The number of clusters that yields the highest V-measure is taken as the optimal number of clusters.\n",
    "\n",
    "Here is a rough outline of the procedure in pseudocode:\n",
    "\n",
    "1)Initialize a variable max_v_measure to 0 and optimal_clusters to 0.\n",
    "2)For each possible number of clusters (from 2 to some reasonable upper limit):\n",
    "2.1)Run the clustering algorithm with the current number of clusters.\n",
    "2.2)Compute the V-measure of the resulting clustering.\n",
    "2.3)If the current V-measure is greater than max_v_measure, update max_v_measure to the current V-measure and optimal_clusters to the current number of clusters.\n",
    "3)At the end of this process, optimal_clusters will hold the number of clusters that yielded the highest V-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a219c-bb7e-4708-90a1-d5e3bcb0a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59fd836a-5f14-4693-8115-e48cbe22920b",
   "metadata": {},
   "source": [
    "Q7. What are some advantages and disadvantages of using the Silhouette Coefficient to evaluate a clustering result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dabb2-d7ee-48ec-83e3-25341a2c0d54",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is a commonly used metric for evaluating the quality of a clustering result. It measures how close each point in one cluster is to the points in the neighboring clusters.\n",
    "\n",
    "\n",
    "Advantages of the Silhouette Coefficient:\n",
    "\n",
    "Intuitive Interpretation: The Silhouette Coefficient values range between -1 and 1, which is easy to interpret. A high value indicates that the sample is well clustered, a small value indicates that the sample is incorrectly clustered, and around zero indicates overlapping clusters.\n",
    "\n",
    "Doesn't Require Labels: The Silhouette Coefficient is an internal evaluation metric, meaning it does not require true labels of data points. This makes it useful for evaluating unsupervised learning algorithms like clustering.\n",
    "\n",
    "Visualization of Clusters: The Silhouette Coefficient can be visualized for each sample or data point, which can provide valuable insights about the cohesion and separation of the formed clusters.\n",
    "\n",
    "\n",
    "Disadvantages of the Silhouette Coefficient:\n",
    "\n",
    "Assumes Convex Clusters: The Silhouette Coefficient assumes that clusters are convex and isotropic, which might not be the case with real-world data. This can lead to poor or misleading Silhouette scores for good clusterings when the clusters are elongated, irregularly shaped, or have varying density.\n",
    "\n",
    "Sensitivity to Noise and Outliers: The Silhouette Coefficient is sensitive to noise and outliers. A small amount of noise can alter the distances between clusters and data points, which can significantly affect the Silhouette score.\n",
    "\n",
    "Computational Complexity: The Silhouette Coefficient can be computationally expensive to calculate, especially for large datasets, as it involves calculating distances between all pairs of points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a531f5-38c5-4d31-8a6a-04b711b465dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38829731-72e0-4a76-84ac-3bbb316b8710",
   "metadata": {},
   "source": [
    "Q8. What are some limitations of the Davies-Bouldin Index as a clustering evaluation metric? How can they be overcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7f27a-0795-41de-ad83-168038dd7471",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index (DBI) is a measure of the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. While the DBI can be a useful metric, it has several limitations:\n",
    "\n",
    "Assumption of Convex Clusters: The DBI assumes that clusters are convex and isotropic. This can lead to misleading scores when clusters have complex shapes or different densities.\n",
    "\n",
    "Sensitivity to Noise and Outliers: Like many clustering metrics, the DBI can be sensitive to noise and outliers. Outliers can cause a cluster to appear larger or more spread out than it actually is, which can inflate the DBI.\n",
    "\n",
    "Lack of Scale Invariance: The DBI isn't scale-invariant. Scaling the data can affect the DBI value, which can make it difficult to compare DBI values across different datasets or feature sets.\n",
    "\n",
    "\n",
    "To overcome these limitations:\n",
    "\n",
    "Data Preprocessing: Outliers can be detected and removed or imputed before clustering to reduce their impact on the DBI. Data can also be normalized or standardized to achieve scale invariance.\n",
    "\n",
    "Alternative Metrics: Other metrics like the Silhouette Coefficient or the Calinski-Harabasz Index can be used. These metrics make different assumptions and may be less sensitive to certain issues.\n",
    "\n",
    "Multiple Metrics: Using multiple metrics in combination can give a more holistic view of the clustering quality. Different metrics can capture different aspects of the clustering, and looking at several metrics together can help identify strengths and weaknesses that aren't apparent when looking at any single metric alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559c9ef-ee2f-455b-b5fc-4769ac602faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf42853-dfef-4b77-a6e4-ed1ab17877aa",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between homogeneity, completeness, and the V-measure? Can they have different values for the same clustering result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6fce1-a9a8-4d73-8758-6512c987396d",
   "metadata": {},
   "source": [
    "Homogeneity, completeness, and the V-measure are external metrics used to evaluate the quality of a clustering result when true labels are known.\n",
    "\n",
    "Homogeneity: A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "\n",
    "Completeness: A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "\n",
    "V-measure: The V-measure is the harmonic mean of homogeneity and completeness. It provides a balanced measure that can be used to evaluate the overall quality of the clustering, especially when both homogeneity and completeness are important.\n",
    "\n",
    "The V-measure is defined as:\n",
    "\n",
    "V = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "\n",
    "All three metrics can take on values between 0 and 1, with 1 being the ideal score. However, for a given clustering result, homogeneity, completeness, and the V-measure can indeed have different values. For example, it's possible to have a high homogeneity but low completeness if each cluster contains only a single class, but a single class is scattered across multiple clusters. Similarly, you can have high completeness but low homogeneity if all members of a single class are in one cluster, but the cluster contains members from multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2008d-e4d0-44d0-8264-4271db9ef06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b41e0e-399c-4aa9-906c-785aff588996",
   "metadata": {},
   "source": [
    "Q10. How can the Silhouette Coefficient be used to compare the quality of different clustering algorithms on the same dataset? What are some potential issues to watch out for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dfddf-1f50-44b2-9476-f62c9923b4ee",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is a measure of how well each datapoint fits within its assigned cluster and how poorly it fits into neighboring clusters. It can be used to compare the performance of different clustering algorithms on the same dataset by following these steps:\n",
    "\n",
    "Run each of the clustering algorithms on the dataset and obtain the cluster assignments.\n",
    "For each set of cluster assignments, calculate the Silhouette Coefficient.\n",
    "The algorithm that yields the highest Silhouette Coefficient is the one that has, on average, done the best job of clustering the data.\n",
    "However, there are a few issues to watch out for when using the Silhouette Coefficient to compare clustering algorithms:\n",
    "\n",
    "Assumptions: The Silhouette Coefficient assumes that clusters are convex and isotropic, meaning that they are roughly round and similar in size. Clustering algorithms that identify non-convex or irregularly shaped clusters, such as DBSCAN, might produce meaningful clusters that have low Silhouette Coefficients.\n",
    "\n",
    "Number of Clusters: The number of clusters can greatly affect the Silhouette Coefficient. If the number of clusters is not set appropriately for each algorithm, it can skew the comparison. In such cases, it can be helpful to use methods like the Elbow method or the Gap statistic to estimate the optimal number of clusters for each algorithm.\n",
    "\n",
    "Noise and Outliers: The Silhouette Coefficient is sensitive to noise and outliers. Clustering algorithms that are more robust to outliers might produce lower Silhouette Coefficients, even though their cluster assignments may be more meaningful.\n",
    "\n",
    "Scale Sensitivity: The Silhouette Coefficient is sensitive to the scale of the features. If the features aren't appropriately normalized or standardized, it can affect the distance calculations and lead to misleading Silhouette Coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d53c78-2c4e-4e05-bbb0-c0e9358c698f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfdc1b0d-425d-494a-9f3d-8253502e4b29",
   "metadata": {},
   "source": [
    "Q11. How does the Davies-Bouldin Index measure the separation and compactness of clusters? What are some assumptions it makes about the data and the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0316dc7-d48a-4289-8f39-9d30e0dd99e1",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index (DBI) measures the quality of a clustering by considering both the separation and compactness of the clusters.\n",
    "\n",
    "Compactness: Compactness is a measure of how close the elements of a cluster are. The more compact a cluster, the closer its elements are to each other. DBI measures compactness as the average distance between each element of a cluster and the centroid of that cluster.\n",
    "\n",
    "Separation: Separation is a measure of how far apart different clusters are. The larger the separation, the further apart the clusters are. DBI measures separation as the distance between the centroids of two clusters.\n",
    "\n",
    "The DBI for a pair of clusters is a ratio of the sum of their compactness to their separation. The overall DBI for a clustering is the average DBI for all pairs of clusters. Therefore, a lower DBI corresponds to a better clustering because it means that clusters are compact and well-separated.\n",
    "\n",
    "\n",
    "The DBI makes several assumptions about the data and the clusters:\n",
    "\n",
    "Spherical Clusters: The DBI assumes that clusters are convex and isotropic, which roughly translates to being spherical in shape. This is due to the use of centroid distance for measuring both compactness and separation.\n",
    "\n",
    "Equal Cluster Sizes: The DBI may not perform well if the clusters have widely differing sizes, as it does not take into account the size of the clusters in the calculation.\n",
    "\n",
    "Equal Cluster Densities: The DBI assumes equal density among clusters. If this assumption is violated, as is often the case with real-world data, the DBI might not provide an accurate evaluation of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05d02d-f877-44e4-bbfd-0c8581a1ed38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea7893d-f248-4846-a62d-7e4942328dcd",
   "metadata": {},
   "source": [
    "Q12. Can the Silhouette Coefficient be used to evaluate hierarchical clustering algorithms? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80b5c-1068-4573-abbd-6740d4d75734",
   "metadata": {},
   "source": [
    " Yes, the Silhouette Coefficient can certainly be used to evaluate hierarchical clustering algorithms.\n",
    "\n",
    "Hierarchical clustering algorithms produce a tree-like clustering structure called a dendrogram. To use the Silhouette Coefficient, you first need to determine a cut-off point in the dendrogram to specify the final number of clusters.\n",
    "\n",
    "Once the clusters are defined, you can calculate the Silhouette Coefficient in the same way as for any other type of clustering:\n",
    "\n",
    "1)For each data point, calculate the average distance between the point and all other points in the same cluster (a). This measures how well the point fits within its own cluster.\n",
    "2)For the same data point, find the minimum average distance to points in other clusters (b). This measures how poorly the point fits within the closest neighboring cluster.\n",
    "3)The Silhouette Coefficient for the data point is then given by (b - a) / max(a, b).\n",
    "The overall Silhouette Coefficient for the clustering is the average Silhouette Coefficient across all data points.\n",
    "\n",
    "One advantage of using the Silhouette Coefficient with hierarchical clustering is that you can calculate it for different cut-off points to help determine the optimal number of clusters. For each possible number of clusters (from 2 up to some reasonable limit), you cut the dendrogram at the corresponding point, calculate the Silhouette Coefficient, and choose the number of clusters that gives the highest Silhouette Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ad105-af91-462e-91a0-47bbe382b04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6a31f-4af3-4c8d-b4da-21d788969ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
